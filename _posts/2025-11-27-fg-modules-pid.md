---
layout: post

title: "Finitely generated modules over PID (the right way)"

date: 2025-11-27
---

$$
\newcommand{\nc}[3]{\newcommand{#2}[#1]{#3}}
\nc0{\(}{\left(}
\nc0{\)}{\right)}
\nc0{F}{\mathbb{F}}
\nc0{s}{\sigma}
\nc0{End}{\text{End}}
\nc0{adj}{\text{adj}}
\nc0{Tr}{\text{Tr}}
\nc0{Wedge}{\bigwedge}
\nc0{^}{\wedge}
\nc0{mto}{\mapsto}
\nc0{a}{\alpha}
\nc2{Hom}{\text{Hom}\(#1, #2\)}
\nc0{\ox}{\otimes}
$$

I think that a lot of the time, the proof people hear for this theorem is not very good.
<!-- At least this is true for the proof I first saw. -->
There is an issue with assessing such a thing,
since after knowing the right proof it is impossible to actually read inferior proofs,
but I see that many proofs of this theorem:

1. Do not show a big matrix
2. Treat it as a theorem, rather than an algorithm on said matrix

And that is wrong.

OK, what does the theorem say?
We have a principal ideal domain $R$.
<!-- (a ring in which every ideal is generated by a single element). -->
We also have a finitely generated $R$-module $M$.
It turns out that $M$ must have the form $R/I_1 \oplus R/I_2 \oplus \dots \oplus R/I_n$
for some ideals $I_1, \dots, I_n$ in $R$.
The theorem says some more things but this is the main one.

For the case $R = \mathbb{Z}$ this means that every finitely generated abelian group
is a direct sum of finitely many cyclic groups.
(If you don't know ring theory
replace every instance of $R$ with $\mathbb{Z}$ and every instance of $M$
with the finitely generated abelian group, you won't miss out on anything)

Since $M$ is finitely generated, we can define a surjective
homomorphism $\phi : R^n \to M$ by $(r_1, \dots , r_n) \mapsto r_1m_1 + \dots + r_nm_n$.
Let $K$ be the kernel of $\phi$.
It turns out that every submodule of $R^n$
is finitely generated. This is shown in every proof for the structure theorem
and I'll just assume this fact.
<!-- It turns out that $K$ is finitely generated, let's postpone the proof of this fact. -->
It follows that there is a homomorphism $f : R^m \to R^n$ whose image is $K$.

This homomorphism determines $M$, because $M$ is simply $R^n/im(f)$.
We can write it in a matrix form:

$$\pmatrix{
    a_{11}&a_{12}&\dots&a_{1m}\\ 
    a_{21}&a_{22}&\dots&a_{2m}\\ 
    \dots&\dots&\dots&\dots\\
    a_{n1}&a_{n2}&\dots&a_{nm}\\ 
} : R^m \to R^n.$$

If this matrix was diagonal, then the result of the theorem would be clear;
the quotient $R^m / im(f)$ would factor as a direct sum of terms that are either $R/a_{ii}R$ or just $R$.
Note that a non-square matrix can be diagonal just fine, it will simply have some rows or columns that have to be filled with zeros.

Let's take an automorphism $\phi : R^n \to R^n$ (i.e. an element of $GL_n(R)$).
<!-- Let's take two distinct indices $1 \le i,j \le n$ and an element $\lambda$ in $R$. 
Then there is an automorphism $\phi_{i,j,\lambda} : R^n \to R^n$
defined by 
$$\phi_{i,j,\lambda}(r_1, \dots, r_n) = (r_1, \dots, r_{i - 1}, r_i + \lambda r_j, r_{i + 1}, \dots, r_n).$$ -->
Replacing the linear map $f$ by the composition $f \circ \phi$
has absolutely no effect on $M$; indeed, the two maps $f$ and $f \circ \phi$
have exactly the same image so they define the same $M$. But this composition does have an effect on the matrix that represents $f$; 
it multiplies it by an invertible $n \times n$ matrix from the right.

<!-- (either that or the other direction.) -->

There are also automorphisms $\phi : R^m \to R^m$
 <!-- that add one coordinate, multiplied by some $\lambda \in R$, -->
<!-- to another coordinate. -->
and we can replace $f$ with $\phi \circ f$,
which corresponds to multiplying our matrix by an invertible $m \times m$ matrix from the left.
This is very nice, but this time the composition does change the image of $f$,
so it doesn't really leave $M$ intact.
However it is not too hard to see that the new quotient $M$ resulting from this operation is isomorphic to the original.

OK, to summarize, we have an $n \times m$ matrix with entries in $R$,
we are allowed to multiply it by invertible matrices on both sides, and we need to turn it into a diagonal matrix.
This is much more fun.

<!-- Let's look at the first row of our matrix. -->
Let's give the matrix a name, $A$.
Let's look at the top-left element of $A$ and try to make it as small as possible
(meaning that the ideal $Ra$ is as large as possible;
if you are thinking about abelian groups, ignore this you're doing great).

Let's look at the first row of the matrix.
We have $a$ on the corner, and let's take some other element $b$.

$$\pmatrix{
    a&\star&b&\star&\star\\
    \dots&\dots&\dots&\dots&\dots
}$$

<!-- In the case $R = \mathbb{Z}$ -->
<!-- we can immediately apply the Euclidean algorithm; -->
Our operations include the column operations 
that allow us to replace the pair $(a, b)$ with any of the following:

1. $(a, b) \mapsto (a - \lambda b, b)$
2. $(a, b) \mapsto (a, b - \lambda a)$
3. $(a, b) \mapsto (b, a)$.

In the case $R = \mathbb{Z}$
we can immediately perform the Euclidean algorithm to reduce $(a, b)$ into $(\gcd(a, b), 0)$.

For general $R$ this doesn't quite work;
we are given $s,t$ such that $sa + tb = \gcd(a, b)$
but it might not be possible to get to the $\gcd$
using only the reduction steps above.
However, we can use the invertible $2 \times 2$ matrix

$$
\pmatrix{
    s & t \\
     -\frac{b}{\gcd(a,b)} & \frac{a}{\gcd(a,b)}

}$$

to replace $(a, b)$ with $(\gcd(a, b), 0)$ anyways.

Now we can go over every element in the first row and $\gcd$-squish it into the first one,
until the first element is the total $\gcd$ of the row and all the other elements are zero
(the first element might start out as $0$, but as long as there is some nonzero element in the row
we can switch them. Of course, $0$ is the largest element in $R$, we don't want it in the corner).

Now we can look at the first column of $A$.
And we can do exactly the same process to it, this time using row operations.
Eventually, the top-left corner would become the $\gcd$ of the entire first column,
and all the other elements will be $0$. However, those operations
messed up the first row that we cleaned previously! We have to go back to clean the row,
then clean the column again, etc.
This process has to end because the top-left corner is becoming smaller in each step.

$$
\pmatrix{
    a_0 & \star & \star \\
    \star & \star & \star \\
    \star & \star & \star \\
    \star & \star & \star
}, \quad
\pmatrix{
    a_1 & 0 & 0 \\
    \star & \star & \star \\
    \star & \star & \star \\
    \star & \star & \star
}, \quad
\pmatrix{
    a_2 & \star & \star \\
    0 & \star & \star \\
    0 & \star & \star \\
    0 & \star & \star
}, \quad
\pmatrix{
    a_3 & 0 & 0 \\
    \star & \star & \star \\
    \star & \star & \star \\
    \star & \star & \star
}, \quad
\dots
$$

Eventually, all values in the first row and column,
apart from the corner, would be zero.
When this stage is reached,
we can start doing the same algorithm on the smaller $(n - 1) \times (m - 1)$ matrix inside,
and it won't ruin the clean state of the first and second columns:

$$
\pmatrix{
    a & 0 & 0 \\
    0 & b_0 & \star \\
    0 & \star & \star \\
    0 & \star & \star
}, \quad
\pmatrix{
    a & 0 & 0 \\
    0 & b_1 & 0 \\
    0 & \star & \star \\
    0 & \star & \star
}, \quad
\pmatrix{
    a & 0 & 0 \\
    0 & b_2 & \star \\
    0 & 0 & \star \\
    0 & 0 & \star
}, \quad
\pmatrix{
    a & 0 & 0 \\
    0 & b_3 & 0 \\
    0 & \star & \star \\
    0 & \star & \star
}, \quad
\dots
$$

and so on. Eventually the matrix becomes diagonal
so we finish the proof. 
This thing is called the Smith normal form.

---

The proof is not complete because I used the fact submodules of $R^n$ are finitely generated without proving it.
However, I'll be delighted to tell you that I didn't _technically_ need this fact.

The process described above works just fine for $n \times \infty$ matrices.
Given an infinite row,
we can perform a finite number of operations until the left-most element is equal to the $\gcd$
of the entire row,
<!-- such that it divides the (possibly infinitely many) other elements left. -->
and then in a single operation reduce every other element to $0$.
This cleans the infinite row in finitely many steps so it lets us turn $A$ into diagonal form in finitely many steps.
It works for any type of infinity, although it is awkward to visualize an $n \times 2^{\aleph_0}$ matrix.

This process however does not work for $\infty \times \infty$ matrices;
if it did, $\mathbb{Q}$ would be a direct sum of cyclic groups.
With $\infty \times \infty$ matrices,
it is possible to clean every finite collection of rows and colunms in a finite number of steps,
but not the entire matrix.

<!-- 
I'm returning to the unproven claim that a submodule $K \le R^n$ is finitely generated.
Technically we don't really need it as the process described above also works for an $n \times \infty$ matrix
(though not for $\infty \times \infty$!). -->






<!-- (An $R$-module is an abelian group
whose elements can be multiplied by scalars from $R$
(this is a straightforward generalization of the notion of a vector space
whose elements can be multiplied by scalars from a field);
To say that $M$ is finitely generated means that there are $m_1, \dots, m_n \in M$
such that every element in $M$ has the form $r_1m_1 + \dots + r_n m_n$
(just like a finite-dimensional vector space over a field)) -->